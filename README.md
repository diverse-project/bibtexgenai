# DiverSE publications and generative AI

The goal is to illustrate our works/publications with punchlines (summary?) and pictures (for replacing the good old tag clouds). 

There is right now a notebook that gets DiverSE bibtex entries, and retrieves abstracts/authors/titles out of HAL ids. 
Or even better there is a CSV file (and a .bib) in such a way you can start associating something to each entry. 
Originally, the code synthesized an "extreme summarization" (two sentences) ouf of the abstract, leveraging GPT-2 like technology. 

We can now envision to be more ambitious:
 * synthesize "better" summaries (better: more to the point, more engaged, etc.)
 * synthesize prompts that could be fed to midjourney or stabledifussion or whatever

short example, without any automation

abstract:
```
Cyber security threats are important and growing issues in computing systems nowadays. Among them are the side-channel attacks, made possible by information leaking from computing systems through nonfunctional properties like execution time, consumed energy, power profiles, etc. These attacks are especially difficult to protect from, since they rely on physical measurements not usually envisioned when designing the functional properties of a program. Furthermore, countermeasures are usually dedicated to protect a particular program against a particular attack, lacking universality. To help fight these threats, we propose in this paper the Indiscernibility Methodology, a novel methodology to quantify with no prior knowledge the information leaked from programs, thus providing the developer with valuable security metrics, derived either from topology or from information theory. Our original approach considers the code to be analyzed as a completely black box, only the public inputs and leakages being observed. It can be applied to various types of side-channel leakages: time, energy, power, EM, etc. In this paper, we first present our Indiscernibility Methodology, including channels of information and our threat model. We then detail the computation of our novel metrics, with strong formal foundations based both on topological security (with distances defined between secret-dependent observations) and on information theory (quantifying the remaining secret information after observation by the attacker). Then we demonstrate the applicability of our approach by providing experimental results for both time and power leakages, studying both average case-, worst case-and indiscernible information metrics.
```

Summary (as generated by ChatGPT-4): 
`This paper introduces the Indiscernibility Methodology, a unique approach to measure the information leakage in programs, providing comprehensive security metrics that can help counteract diverse side-channel cyber attacks, effectively treating the code as a black box and offering results applicable to various types of leakages.`

AI Image Prompt (as generated by ChatGPT-4): 
`"Visualize an opaque black box representing a computer program, with information subtly escaping in various forms such as time, energy, and power, while a shield symbolizing the novel Indiscernibility Methodology quantifies and mitigates these leaks."`

and you can fed this prompt to tools like: https://dreamstudio.ai/generate

Obviously, the challenge is to automate everything and find the good strategies for generating interesting stuff. 

Ideas:
 - It is possible to have a "global" visualization of all publications (in contract to a "per publication" approach)
 - The abstract is not the only source of information (it's possible to analyze the PDF and extract relevant information)  







